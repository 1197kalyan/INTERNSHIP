{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce0f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaaf2ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf347bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kalya\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c95f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3473759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "web = ('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6d7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = requests.get(web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437c5d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wik = BeautifulSoup(k.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce5fac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_tags = wik.find_all(['h1','h2','h3','h4','h5','h6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6df720",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4bc83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header_tag in header_tags:\n",
    "    header_texts.append(header_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9177a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(header_texts, columns=['Header Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c981f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Text\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df071ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Write a python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) \n",
    "#and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa21d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title    Year\n",
      "0                            The Shawshank Redemption  (1994)\n",
      "1                                       The Godfather  (1972)\n",
      "2                                     The Dark Knight  (2008)\n",
      "3                               The Godfather Part II  (1974)\n",
      "4                                        12 Angry Men  (1957)\n",
      "5                                    Schindler's List  (1993)\n",
      "6       The Lord of the Rings: The Return of the King  (2003)\n",
      "7                                        Pulp Fiction  (1994)\n",
      "8   The Lord of the Rings: The Fellowship of the Ring  (2001)\n",
      "9                     Il buono, il brutto, il cattivo  (1966)\n",
      "10                                       Forrest Gump  (1994)\n",
      "11                                         Fight Club  (1999)\n",
      "12              The Lord of the Rings: The Two Towers  (2002)\n",
      "13                                          Inception  (2010)\n",
      "14                            The Empire Strikes Back  (1980)\n",
      "15                                         The Matrix  (1999)\n",
      "16                                         GoodFellas  (1990)\n",
      "17                    One Flew Over the Cuckoo's Nest  (1975)\n",
      "18                                              Se7en  (1995)\n",
      "19                               Shichinin no samurai  (1954)\n",
      "20                              It's a Wonderful Life  (1946)\n",
      "21                           The Silence of the Lambs  (1991)\n",
      "22                                     Cidade de Deus  (2002)\n",
      "23                                Saving Private Ryan  (1998)\n",
      "24                                       Interstellar  (2014)\n",
      "25                                    La vita è bella  (1997)\n",
      "26                                     The Green Mile  (1999)\n",
      "27                                          Star Wars  (1977)\n",
      "28                         Terminator 2: Judgment Day  (1991)\n",
      "29                                 Back to the Future  (1985)\n",
      "30                      Sen to Chihiro no kamikakushi  (2001)\n",
      "31                                        The Pianist  (2002)\n",
      "32                                             Psycho  (1960)\n",
      "33                                       Gisaengchung  (2019)\n",
      "34                                               Léon  (1994)\n",
      "35                                      The Lion King  (1994)\n",
      "36                                          Gladiator  (2000)\n",
      "37                                 American History X  (1998)\n",
      "38                                       The Departed  (2006)\n",
      "39                                 The Usual Suspects  (1995)\n",
      "40                                       The Prestige  (2006)\n",
      "41                                           Whiplash  (2014)\n",
      "42                                         Casablanca  (1942)\n",
      "43                                            Seppuku  (1962)\n",
      "44                                     Hotaru no haka  (1988)\n",
      "45                                   The Intouchables  (2011)\n",
      "46                                       Modern Times  (1936)\n",
      "47                       Once Upon a Time in the West  (1968)\n",
      "48                                        Rear Window  (1954)\n",
      "49                              Nuovo Cinema Paradiso  (1988)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.imdb.com/chart/top\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "movies = soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "\n",
    "\n",
    "movies_list = []\n",
    "\n",
    "\n",
    "for movie in movies:\n",
    "    movie_title = movie.a.text\n",
    "    movie_year = movie.span.text\n",
    "    \n",
    "    movies_list.append((movie_title, movie_year, ))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(movies_list, columns=[\"Title\", \"Year\"])\n",
    "\n",
    "\n",
    "print(df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae915d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to display IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of \n",
    "#release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65272f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  title  year rating\n",
      "0   Ramayana: The Legend of Prince Rama  1993    8.5\n",
      "1            Rocketry: The Nambi Effect  2022    8.4\n",
      "2                               Nayakan  1987    8.4\n",
      "3                              Gol Maal  1979    8.4\n",
      "4                            Anbe Sivam  2003    8.4\n",
      "5                           777 Charlie  2022    8.4\n",
      "6                              Jai Bhim  2021    8.4\n",
      "7                     Pariyerum Perumal  2018    8.4\n",
      "8                              3 Idiots  2009    8.4\n",
      "9                           Apur Sansar  1959    8.4\n",
      "10                     Manichitrathazhu  1993    8.4\n",
      "11                                #Home  2021    8.3\n",
      "12                      Soorarai Pottru  2020    8.3\n",
      "13                         Black Friday  2004    8.3\n",
      "14                    Kumbalangi Nights  2019    8.3\n",
      "15                    C/o Kancharapalem  2018    8.3\n",
      "16                     Taare Zameen Par  2007    8.3\n",
      "17                             Kireedam  1989    8.3\n",
      "18                               Dangal  2016    8.3\n",
      "19                               Kaithi  2019    8.3\n",
      "20                               Jersey  2019    8.3\n",
      "21                                   96  2018    8.3\n",
      "22                          Maya Bazaar  1957    8.2\n",
      "23                            Natsamrat  2016    8.2\n",
      "24                               Asuran  2019    8.2\n",
      "25                           Drishyam 2  2021    8.2\n",
      "26                           Sita Ramam  2022    8.2\n",
      "27                         Thevar Magan  1992    8.2\n",
      "28                           Visaaranai  2015    8.2\n",
      "29                  Sarpatta Parambarai  2021    8.2\n",
      "30                           Thalapathi  1991    8.2\n",
      "31                      Pather Panchali  1955    8.2\n",
      "32                         Nadodikkattu  1987    8.2\n",
      "33                             Drishyam  2013    8.2\n",
      "34                   Jaane Bhi Do Yaaro  1983    8.2\n",
      "35                         Thani Oruvan  2015    8.2\n",
      "36                         Sardar Udham  2021    8.2\n",
      "37                            Aparajito  1956    8.2\n",
      "38                         Vada Chennai  2018    8.2\n",
      "39                    Khosla Ka Ghosla!  2006    8.2\n",
      "40                              Anniyan  2005    8.1\n",
      "41                             Ratsasan  2018    8.1\n",
      "42                        Chupke Chupke  1975    8.1\n",
      "43                   Gangs of Wasseypur  2012    8.1\n",
      "44                              Peranbu  2018    8.1\n",
      "45                             Drishyam  2015    8.1\n",
      "46                             Mahanati  2018    8.1\n",
      "47                       Bangalore Days  2014    8.1\n",
      "48                                Satya  1998    8.1\n",
      "49          Agent Sai Srinivasa Athreya  2019    8.1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.imdb.com/india/top-rated-indian-movies/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "movie_table = soup.find(\"tbody\", class_=\"lister-list\")\n",
    "\n",
    "\n",
    "movie_data = []\n",
    "for row in movie_table.find_all(\"tr\"):\n",
    "    movie_dict = {}\n",
    "    title_column = row.find(\"td\", class_=\"titleColumn\")\n",
    "    movie_dict[\"title\"] = title_column.a.text\n",
    "    movie_dict[\"year\"] = title_column.span.text[1:5]\n",
    "    movie_dict[\"rating\"] = row.find(\"td\", class_=\"ratingColumn imdbRating\").strong.text\n",
    "    movie_data.append(movie_dict)\n",
    "\n",
    "df = pd.DataFrame(movie_data)\n",
    "\n",
    "print(df.head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f759c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb0aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pos             \\nTeam\\nT\\n \\nMatches\\nM\\n \\nPoints\\nP\\n  \\\n",
      "0   None                    None           None          None   \n",
      "1      1        \\n\\nIndia\\nIND\\n             44         5,010   \n",
      "2      2    \\n\\nAustralia\\nAUS\\n             32         3,572   \n",
      "3      3   \\n\\nNew Zealand\\nNZ\\n             29         3,229   \n",
      "4      4      \\n\\nEngland\\nENG\\n             33         3,656   \n",
      "5      5     \\n\\nPakistan\\nPAK\\n             25         2,649   \n",
      "6      6  \\n\\nSouth Africa\\nSA\\n             27         2,775   \n",
      "7      7   \\n\\nBangladesh\\nBAN\\n             33         3,129   \n",
      "8      8     \\n\\nSri Lanka\\nSL\\n             34         2,976   \n",
      "9      9  \\n\\nAfghanistan\\nAFG\\n             20         1,419   \n",
      "10    10   \\n\\nWest Indies\\nWI\\n             41         2,902   \n",
      "\n",
      "                                        \\nRating\\nR\\n  \n",
      "0                                                None  \n",
      "1   \\n                            114\\n           ...  \n",
      "2                                                 112  \n",
      "3                                                 111  \n",
      "4                                                 111  \n",
      "5                                                 106  \n",
      "6                                                 103  \n",
      "7                                                  95  \n",
      "8                                                  88  \n",
      "9                                                  71  \n",
      "10                                                 71  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    \n",
    "    headers = [header.text for header in table.find_all('th')]\n",
    "    \n",
    "    \n",
    "    rows = []\n",
    "    for row in table.find_all('tr'):\n",
    "        rows.append([val.text for val in row.find_all('td')])\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    \n",
    "    \n",
    "    print(df.head(11))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68ef054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Top 10 ODI Batsmen along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25d50432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                rank                   name  \\\n",
      "0       2\\n                                \\n\\n\\n(0)  Rassie van der Dussen   \n",
      "1  3\\n                                \\n\\n\\n\\n\\n(...           David Warner   \n",
      "2  4\\n                                \\n\\n\\n\\n\\n(...        Quinton de Kock   \n",
      "3       5\\n                                \\n\\n\\n(0)            Imam-ul-Haq   \n",
      "4       6\\n                                \\n\\n\\n(0)           Shubman Gill   \n",
      "5       7\\n                                \\n\\n\\n(0)            Virat Kohli   \n",
      "6       8\\n                                \\n\\n\\n(0)            Steve Smith   \n",
      "7       9\\n                                \\n\\n\\n(0)           Rohit Sharma   \n",
      "8      10\\n                                \\n\\n\\n(0)        Kane Williamson   \n",
      "\n",
      "  team rating  \n",
      "0   SA    787  \n",
      "1  AUS    747  \n",
      "2   SA    743  \n",
      "3  PAK    740  \n",
      "4  IND    734  \n",
      "5  IND    727  \n",
      "6  AUS    719  \n",
      "7  IND    719  \n",
      "8   NZ    700  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "rankings_table = soup.find(\"table\", attrs={\"class\": \"table\"})\n",
    "\n",
    "\n",
    "players = []\n",
    "for row in rankings_table.find_all(\"tr\"):\n",
    "    player_info = {}\n",
    "    cols = row.find_all(\"td\")\n",
    "    if len(cols) > 2:\n",
    "        player_info[\"rank\"] = cols[0].text.strip()\n",
    "        player_info[\"name\"] = cols[1].text.strip()\n",
    "        player_info[\"team\"] = cols[2].text.strip()\n",
    "        player_info[\"rating\"] = cols[3].text.strip()\n",
    "        players.append(player_info)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(players)\n",
    "\n",
    "print(df.head(11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4facd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2ddcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Pos  \\\n",
      "0                                               None   \n",
      "1  \\n\\n\\n                            1\\n         ...   \n",
      "2  \\n\\n\\n                                    2\\n ...   \n",
      "3  \\n\\n\\n                                    3\\n ...   \n",
      "4  \\n\\n\\n                                    4\\n ...   \n",
      "5  \\n\\n\\n                                    5\\n ...   \n",
      "6  \\n\\n\\n                                    6\\n ...   \n",
      "7  \\n\\n\\n                                    7\\n ...   \n",
      "8  \\n\\n\\n                                    8\\n ...   \n",
      "9  \\n\\n\\n                                    9\\n ...   \n",
      "\n",
      "                                 Player           Team   Rating  \\\n",
      "0                                  None           None     None   \n",
      "1  \\n\\n\\n\\n\\n\\n\\n\\nMohammed Siraj\\n\\n\\n  \\n\\n\\nIND\\n\\n  \\n729\\n   \n",
      "2                    \\nJosh Hazlewood\\n      \\n\\nAUS\\n      727   \n",
      "3                       \\nTrent Boult\\n       \\n\\nNZ\\n      708   \n",
      "4                    \\nMitchell Starc\\n      \\n\\nAUS\\n      665   \n",
      "5                       \\nRashid Khan\\n      \\n\\nAFG\\n      659   \n",
      "6                        \\nAdam Zampa\\n      \\n\\nAUS\\n      655   \n",
      "7                   \\nShakib Al Hasan\\n      \\n\\nBAN\\n      652   \n",
      "8                    \\nShaheen Afridi\\n      \\n\\nPAK\\n      641   \n",
      "9                 \\nMustafizur Rahman\\n      \\n\\nBAN\\n      638   \n",
      "\n",
      "                                  Career Best Rating  \n",
      "0                                               None  \n",
      "1  \\n\\n\\n                                736 v Ne...  \n",
      "2  \\n                                733 v Englan...  \n",
      "3  \\n                                775 v Austra...  \n",
      "4  \\n                                783 v New Ze...  \n",
      "5  \\n                                806 v Pakist...  \n",
      "6  \\n                                655 v Englan...  \n",
      "7  \\n                                717 v Zimbab...  \n",
      "8  \\n                                688 v West I...  \n",
      "9  \\n                                695 v West I...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "r = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "\n",
    "table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "\n",
    "\n",
    "headers = [header.text for header in table.find_all(\"th\")]\n",
    "\n",
    "\n",
    "rows = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    rows.append([cell.text for cell in row.find_all(\"td\")])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "#\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bca12332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 ODI teams in women’s cricket along w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4073a880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Pos           Team\\nT Matches\\nM Points\\nP Rating\\nR\n",
      "0  None              None       None      None      None\n",
      "1     1    Australia\\nAUS         21     3,603       172\n",
      "2     2      England\\nENG         28     3,342       119\n",
      "3     3  South Africa\\nSA         26     3,098       119\n",
      "4     4        India\\nIND         27     2,820       104\n",
      "5     5   New Zealand\\nNZ         25     2,553       102\n",
      "6     6   West Indies\\nWI         27     2,535        94\n",
      "7     7   Bangladesh\\nBAN         13       983        76\n",
      "8     8     Thailand\\nTHA          8       572        72\n",
      "9     9     Pakistan\\nPAK         27     1,678        62\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "table = soup.find(\"table\", attrs={\"class\": \"table\"})\n",
    "\n",
    "\n",
    "headers = [header.text.strip() for header in table.find_all(\"th\")]\n",
    "\n",
    "\n",
    "rows = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    rows.append([cell.text.strip() for cell in row.find_all(\"td\")])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "\n",
    "df = df.iloc[:10, :]\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8867842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2296c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank                    Team Rating\n",
      "0    1    \\n\\nAustralia\\nAUS\\n     21\n",
      "1    2      \\n\\nEngland\\nENG\\n     28\n",
      "2    3  \\n\\nSouth Africa\\nSA\\n     26\n",
      "3    4        \\n\\nIndia\\nIND\\n     27\n",
      "4    5   \\n\\nNew Zealand\\nNZ\\n     25\n",
      "5    6   \\n\\nWest Indies\\nWI\\n     27\n",
      "6    7   \\n\\nBangladesh\\nBAN\\n     13\n",
      "7    8     \\n\\nThailand\\nTHA\\n      8\n",
      "8    9     \\n\\nPakistan\\nPAK\\n     27\n",
      "9   10     \\n\\nSri Lanka\\nSL\\n      8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "\n",
    "\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "\n",
    "rows = rows[1:]\n",
    "\n",
    "\n",
    "rankings = []\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "   \n",
    "    cells = row.find_all(\"td\")\n",
    "    \n",
    "    \n",
    "    rank = cells[0].text\n",
    "    team = cells[1].text\n",
    "    rating = cells[2].text\n",
    "    \n",
    "    \n",
    "    rankings.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Team\": team,\n",
    "        \"Rating\": rating\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rankings)\n",
    "\n",
    "df= df.iloc[:10, :]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d353c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bb0b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Player              Team  Rating\n",
      "0      1    Australia\\nAUS      21\n",
      "1      2      England\\nENG      28\n",
      "2      3  South Africa\\nSA      26\n",
      "3      4        India\\nIND      27\n",
      "4      5   New Zealand\\nNZ      25\n",
      "5      6   West Indies\\nWI      27\n",
      "6      7   Bangladesh\\nBAN      13\n",
      "7      8     Thailand\\nTHA       8\n",
      "8      9     Pakistan\\nPAK      27\n",
      "9     10     Sri Lanka\\nSL       8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "data = []\n",
    "rows = table.find_all('tr')\n",
    "for row in rows[1:11]:\n",
    "    cols = row.find_all('td')\n",
    "    player = cols[0].text.strip()\n",
    "    team = cols[1].text.strip()\n",
    "    rating = int(cols[2].text.strip())\n",
    "    data.append([player, team, rating])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ccf7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and \n",
    "#make data frame\u0002i) Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1e0629b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21764\\2375272660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mheadlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SecondaryCard-headline\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LatestNews-container\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mheadline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LatestNews-headline\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'headline' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "response = requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "headlines = soup.find_all(\"div\", class_=\"SecondaryCard-headline\")\n",
    "for i in soup.find_all(\"div\",class_=\"LatestNews-container\"):\n",
    "    headline.append(i.find(\"div\",class_=\"LatestNews-headline\")).text\n",
    "\n",
    "\n",
    "headline_texts = [headline.text for headline in headlines]\n",
    "\n",
    "df = pd.DataFrame({\"Headline\" })\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef823daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Headline  Time\n",
      "0      None  None\n",
      "1      None  None\n",
      "2      None  None\n",
      "3      None  None\n",
      "4      None  None\n",
      "5      None  None\n",
      "6      None  None\n",
      "7      None  None\n",
      "8      None  None\n",
      "9      None  None\n",
      "10     None  None\n",
      "11     None  None\n",
      "12     None  None\n",
      "13     None  None\n",
      "14     None  None\n",
      "15     None  None\n",
      "16     None  None\n",
      "17     None  None\n",
      "18     None  None\n",
      "19     None  None\n",
      "20     None  None\n",
      "21     None  None\n",
      "22     None  None\n",
      "23     None  None\n",
      "24     None  None\n",
      "25     None  None\n",
      "26     None  None\n",
      "27     None  None\n",
      "28     None  None\n",
      "29     None  None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "\n",
    "articles = soup.find_all(\"span\", class_=\"LatestNews-wrapper\")\n",
    "\n",
    "\n",
    "news_data = []\n",
    "\n",
    "\n",
    "for article in articles:\n",
    "    headline = article.find(\"div\",class_=\"LatestNews-container\")\n",
    "for article in articles:\n",
    "    time=article.find(\"div\",class_=\"LatestNews-timestamp\")\n",
    "    \n",
    "    news_data.append({\"Headline\": headline, \"Time\": time})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(news_data)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9) Write a python program to scrape mentioned details from dineout.co.in and make data frame 1) restautant name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a748f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a257781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kalya\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bca718f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kalya\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a202cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d176366c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a analytics-action=\"RestaurantCardClick\" analytics-label=\"86792_Castle Barbeque\" class=\"restnt-name ellipsis\" data-w-onclick=\"sendAnalyticsCommon|w1-restarant\" href=\"/delhi/castle-barbeque-connaught-place-central-delhi-86792\">Castle Barbeque</a>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.content)\n",
    "first_title = soup.find('a',class_=\"restnt-name ellipsis\")\n",
    "first_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf4d3363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Castle Barbeque'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a5cac58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connaught Place, Central Delhi'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc= soup.find('div',class_=\"restnt-loc ellipsis\" )\n",
    "loc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3e47e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jungle Jamboree']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    titles.append(i.text)\n",
    "titles[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4235c0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Castle Barbeque',\n",
       " 'Jungle Jamboree',\n",
       " 'Cafe Knosh',\n",
       " 'Castle Barbeque',\n",
       " 'The Barbeque Company',\n",
       " 'India Grill',\n",
       " 'Delhi Barbeque',\n",
       " 'The Monarch - Bar Be Que Village',\n",
       " 'Indian Grill Room']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14059451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Connaught Place, Central Delhi',\n",
       " '3CS Mall,Lajpat Nagar - 3, South Delhi',\n",
       " 'The Leela Ambience Convention Hotel,Shahdara, East Delhi',\n",
       " 'Pacific Mall,Tagore Garden, West Delhi',\n",
       " 'Gardens Galleria,Sector 38A, Noida',\n",
       " 'Hilton Garden Inn,Saket, South Delhi',\n",
       " 'Taurus Sarovar Portico,Mahipalpur, South Delhi',\n",
       " 'Indirapuram Habitat Centre,Indirapuram, Ghaziabad',\n",
       " 'Suncity Business Tower,Golf Course Road, Gurgaon']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Location = []\n",
    "for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    Location.append(i.text)\n",
    "    \n",
    "Location    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d036cdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'₹ 2,000 for 2 (approx) | Chinese, North Indian'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "price= soup.find('span',class_=\"double-line-ellipsis\")\n",
    "price.text\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06668bb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'₹ 2,000 for 2 (approx) '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = soup.find('span',class_=\"double-line-ellipsis\")\n",
    "price.text.split('|')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dd55437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connaught Place, Central Delhi'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc= soup.find('div',class_=\"restnt-loc ellipsis\" )\n",
    "loc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6eea7fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'₹ 2,000 for 2 (approx) | Chinese, North Indian'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k= soup.find('span',class_=\"double-line-ellipsis\")\n",
    "k.text.split('1')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea9ff363",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/p/m/p59633-166088382462ff137009010.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/p/k/p79307-16051787755fad1597f2bf9.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/d/i/p52501-1661855212630de5eceb6d2.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/f/p549-165000147262590640c0afc.jpg?tr=tr:n-medium']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for i in soup.find_all(\"img\", class_=\"no-img\"):\n",
    "    images.append(i.get('data-src'))\n",
    "images    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a790c934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 5 9 9\n"
     ]
    }
   ],
   "source": [
    "print(len(titles),len(price),len(Location),len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e019a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbe21e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titles</th>\n",
       "      <th>Location</th>\n",
       "      <th>Image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Titles  \\\n",
       "0                   Castle Barbeque   \n",
       "1                   Jungle Jamboree   \n",
       "2                        Cafe Knosh   \n",
       "3                   Castle Barbeque   \n",
       "4              The Barbeque Company   \n",
       "5                       India Grill   \n",
       "6                    Delhi Barbeque   \n",
       "7  The Monarch - Bar Be Que Village   \n",
       "8                 Indian Grill Room   \n",
       "\n",
       "                                            Location  \\\n",
       "0                     Connaught Place, Central Delhi   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi   \n",
       "4                 Gardens Galleria,Sector 38A, Noida   \n",
       "5               Hilton Garden Inn,Saket, South Delhi   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon   \n",
       "\n",
       "                                           Image_url  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Titles':titles,'Location':Location,'Image_url':images})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d11d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "#days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#Scrape below mentioned details and make data frame\u0002i) Paper Title\n",
    "#ii) Authors\n",
    "#iii) Published Date\n",
    "#iv) Paper UR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d9ea47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "   \n",
    "    article_elements = soup.find_all(\"div\", class_=\"sc-orwwe2-2 iquSkg\")\n",
    "\n",
    "   \n",
    "    articles = []\n",
    "\n",
    "   \n",
    "    for article_element in article_elements:\n",
    "      \n",
    "        title = article_element.find(\"h3\").text.strip()\n",
    "        authors = article_element.find(\"span\", class_=\"sc-1w3fpd7-0 dnCnAO\").text.strip()\n",
    "        date = article_element.find(\"span\", class_=\"sc-1thf9ly-2 dvggWt\").text.strip()\n",
    "        paper_url = article_element.find(\"a\")[\"sc-uosj0-0 cmlOwO\"]\n",
    "\n",
    "        articles.append({\n",
    "            \"Title\": title,\n",
    "            \"Authors\": authors,\n",
    "            \"Published Date\": date,\n",
    "            \"Paper URL\": paper_url\n",
    "        })\n",
    "\n",
    "  \n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "  \n",
    "\n",
    "    print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) \n",
    "#from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bca39783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Term of Office]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "html_content = requests.get(url).content\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "table_rows = soup.find_all(\"div\",class_=\"innerSidebar alignRight\")\n",
    "\n",
    "data = []\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all(\"listing cf\")\n",
    "    row = [i.text for i in td]\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=[\"Name\", \"Term of Office\"])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8a389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
